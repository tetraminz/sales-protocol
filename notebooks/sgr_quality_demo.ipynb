{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SGR Quality: Core Debug\n",
        "\n",
        "Цель ноутбука: быстро проверить качество текущего run и прозрачно разобрать кейсы, где `judge_label=0`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import sqlite3\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "DB_PATH = Path(\"dialogs.db\")\n",
        "if not DB_PATH.exists():\n",
        "    raise FileNotFoundError(\"dialogs.db not found. Run: make init-fresh && make scan\")\n",
        "\n",
        "conn = sqlite3.connect(DB_PATH)\n",
        "conn.row_factory = sqlite3.Row\n",
        "\n",
        "def qdf(sql: str, params: tuple[object, ...] = ()) -> pd.DataFrame:\n",
        "    return pd.read_sql_query(sql, conn, params=params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Run Snapshot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_df = qdf(\n",
        "    \"\"\"\n",
        "    SELECT run_id, status, model, conversation_from, conversation_to,\n",
        "           selected_conversations, messages_count, started_at_utc, finished_at_utc, summary_json\n",
        "    FROM scan_runs\n",
        "    ORDER BY started_at_utc DESC\n",
        "    LIMIT 1\n",
        "    \"\"\"\n",
        ")\n",
        "if run_df.empty:\n",
        "    raise ValueError(\"No scan runs found. Run: make scan\")\n",
        "\n",
        "RUN_ID = str(run_df.loc[0, \"run_id\"])\n",
        "SUMMARY = json.loads(str(run_df.loc[0, \"summary_json\"]) or \"{}\")\n",
        "\n",
        "display(run_df.drop(columns=[\"summary_json\"]))\n",
        "display(pd.DataFrame([{\n",
        "    \"run_id\": RUN_ID,\n",
        "    \"processed\": SUMMARY.get(\"processed\"),\n",
        "    \"inserted\": SUMMARY.get(\"inserted\"),\n",
        "    \"judged\": SUMMARY.get(\"judged\"),\n",
        "    \"skipped_due_to_errors\": SUMMARY.get(\"skipped_due_to_errors\"),\n",
        "    \"metrics_version\": SUMMARY.get(\"metrics_version\"),\n",
        "}]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Rule Quality + Heatmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rule_metrics = qdf(\n",
        "    \"\"\"\n",
        "    SELECT rule_key, judge_correctness, judged_total, judge_true, judge_false\n",
        "    FROM scan_metrics\n",
        "    WHERE run_id=?\n",
        "    ORDER BY rule_key\n",
        "    \"\"\"\n",
        "    , (RUN_ID,)\n",
        ")\n",
        "display(rule_metrics)\n",
        "\n",
        "heatmap = qdf(\n",
        "    \"\"\"\n",
        "    SELECT conversation_id, rule_key,\n",
        "           SUM(CASE WHEN judge_label IS NOT NULL THEN 1 ELSE 0 END) AS judged_total,\n",
        "           SUM(CASE WHEN judge_label=1 THEN 1 ELSE 0 END) AS correct_total,\n",
        "           CASE\n",
        "             WHEN SUM(CASE WHEN judge_label IS NOT NULL THEN 1 ELSE 0 END) = 0 THEN NULL\n",
        "             ELSE 1.0 * SUM(CASE WHEN judge_label=1 THEN 1 ELSE 0 END)\n",
        "                  / SUM(CASE WHEN judge_label IS NOT NULL THEN 1 ELSE 0 END)\n",
        "           END AS score\n",
        "    FROM scan_results\n",
        "    WHERE run_id=?\n",
        "    GROUP BY conversation_id, rule_key\n",
        "    ORDER BY conversation_id, rule_key\n",
        "    \"\"\"\n",
        "    , (RUN_ID,)\n",
        ")\n",
        "display(heatmap.head(20))\n",
        "\n",
        "zone_counts = {\"green\": 0, \"yellow\": 0, \"red\": 0, \"na\": 0}\n",
        "for score in heatmap[\"score\"].tolist():\n",
        "    if pd.isna(score):\n",
        "        zone_counts[\"na\"] += 1\n",
        "    elif score >= 0.9:\n",
        "        zone_counts[\"green\"] += 1\n",
        "    elif score >= 0.8:\n",
        "        zone_counts[\"yellow\"] += 1\n",
        "    else:\n",
        "        zone_counts[\"red\"] += 1\n",
        "display(pd.DataFrame([zone_counts]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Judge-Confirmed Bad Cases (`judge_label=0`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bad_cases = qdf(\n",
        "    \"\"\"\n",
        "    SELECT sr.conversation_id, sr.message_id, sr.rule_key,\n",
        "           sr.eval_hit, sr.judge_expected_hit,\n",
        "           sr.eval_reason_code, sr.eval_reason, sr.judge_rationale,\n",
        "           sr.evidence_quote, sr.eval_confidence, sr.judge_confidence,\n",
        "           ABS(sr.eval_confidence - COALESCE(sr.judge_confidence, 0)) AS confidence_gap,\n",
        "           m.text\n",
        "    FROM scan_results sr\n",
        "    JOIN messages m ON m.message_id = sr.message_id\n",
        "    WHERE sr.run_id=? AND sr.judge_label=0\n",
        "    ORDER BY confidence_gap DESC, sr.rule_key, sr.message_id\n",
        "    \"\"\"\n",
        "    , (RUN_ID,)\n",
        ")\n",
        "print(f\"Всего bad-cases: {len(bad_cases)}\")\n",
        "display(bad_cases.head(25))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Bad Case Drilldown (super clear)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if bad_cases.empty:\n",
        "    print(\"bad-cases не найдено в текущем run\")\n",
        "else:\n",
        "    case = bad_cases.iloc[0]\n",
        "    print(\n",
        "        f\"Выбран кейс: conv={case['conversation_id']} msg={int(case['message_id'])} rule={case['rule_key']} \"\n",
        "        f\"eval_hit={int(case['eval_hit'])} expected_hit={int(case['judge_expected_hit']) if pd.notna(case['judge_expected_hit']) else 'NA'}\"\n",
        "    )\n",
        "\n",
        "    display(pd.DataFrame([case[[\n",
        "        'conversation_id', 'message_id', 'rule_key', 'eval_hit', 'judge_expected_hit',\n",
        "        'eval_reason_code', 'eval_reason', 'judge_rationale', 'evidence_quote',\n",
        "        'eval_confidence', 'judge_confidence', 'confidence_gap'\n",
        "    ]].to_dict()]))\n",
        "\n",
        "    print('Текст сообщения:')\n",
        "    print(str(case['text']))\n",
        "\n",
        "    trace = qdf(\n",
        "        \"\"\"\n",
        "        SELECT phase, attempt, parse_ok, validation_ok, response_http_status, error_message, latency_ms,\n",
        "               request_json, extracted_json\n",
        "        FROM llm_calls\n",
        "        WHERE run_id=? AND message_id=? AND rule_key=?\n",
        "        ORDER BY phase, attempt\n",
        "        \"\"\"\n",
        "        , (RUN_ID, int(case['message_id']), str(case['rule_key']))\n",
        "    )\n",
        "    display(trace[['phase', 'attempt', 'parse_ok', 'validation_ok', 'response_http_status', 'error_message', 'latency_ms']])\n",
        "\n",
        "    for i, row in trace.iterrows():\n",
        "        print(f\"\\n--- TRACE {i + 1}: phase={row['phase']} attempt={int(row['attempt'])} ---\")\n",
        "        try:\n",
        "            req = json.loads(str(row['request_json']))\n",
        "            system = req.get('input', [{}])[0].get('content', '')\n",
        "            user = req.get('input', [{}, {}])[1].get('content', '')\n",
        "            print('SYSTEM PROMPT (first 400 chars):')\n",
        "            print(str(system)[:400])\n",
        "            print('USER PROMPT (first 600 chars):')\n",
        "            print(str(user)[:600])\n",
        "        except Exception as exc:\n",
        "            print(f'failed to parse request_json: {exc}')\n",
        "\n",
        "        print('EXTRACTED JSON (first 600 chars):')\n",
        "        print(str(row['extracted_json'])[:600])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conn.close()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
