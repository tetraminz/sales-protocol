# Case Description: Sales Conversation Audit

## Контекст

В e-commerce и retail-командах у sales есть много коротких консультаций с клиентами. Эти диалоги содержат сигналы о:

- качестве открытия разговора;
- уровне выявления потребностей;
- отработке возражений;
- качестве закрытия и следующего шага.

Ручной аудит таких разговоров дорогой и плохо масштабируется.

## Бизнес-цель

Построить минимальный, но проверяемый pipeline, который на каждый диалог дает:

- детерминированные метрики (прозрачные и тестируемые);
- семантическую LLM-оценку (строго в заранее описанной схеме);
- единый JSONL-формат для последующей аналитики.

## Почему именно `gwenshap/sales-transcripts`

Датасет уже разложен по репликам:

- один CSV-файл = один разговор;
- есть `Chunk_id` (порядок реплик);
- есть `Speaker` и `Text`.

Это позволяет быстро строить evidence-трейс и связывать LLM-вывод с конкретными turn-ами.

## MVP-решение в проекте

Решение делит аналитику на 2 слоя:

1. Deterministic layer:
   - счетчики turn-ов;
   - вопросительные реплики по символу `?`;
   - keyword-сигналы (discount, return policy, shipping, reviews, sizing).
2. LLM layer:
   - scorecard (`opening`, `discovery`, `objection_handling`, `closing`, `overall`);
   - семантические сигналы (`needs`, `objections`, `upsell`, `tone`, `next_step`);
   - evidence по `turn_id + quote`.

Такой split упрощает дебаг: всегда видно, что посчитано "по правилам", а что классифицировано моделью.

## Ожидаемые артефакты

На выходе формируется JSONL, где каждая строка:

- сам диалог в нормализованном виде (`input.turns`, `input.raw_transcript`);
- вычисленные метрики (`computed`);
- строгая LLM-аннотация (`llm`) по JSON Schema.

## Как этим пользоваться в аналитике

Дальше JSONL можно грузить в:

- BI-слой (агрегации по компании, продавцу, неделе);
- quality monitoring (поиск слабых стадий продажного процесса);
- evaluation loops (сравнение LLM-оценки с ручной экспертной).

Примеры метрик второго уровня:

- средний `overall` score по компании;
- доля разговоров с корректным `next_step`;
- доля возражений типа `price`, которые были `handled=yes`.

## Критерии успеха MVP

- JSONL стабильно генерируется для входной выборки;
- unit-тесты закрывают базовый парсинг/compute/OpenAI-запрос;
- при ручной проверке нескольких разговоров evidence-ссылки правдоподобны;
- формат данных пригоден для загрузки в downstream аналитику.

## Риски и как их снижать

- LLM может галлюцинировать:
  - смягчение: strict schema + требование цитат и `turn_id`.
- Различия в формулировках могут ломать keyword-сигналы:
  - смягчение: считать deterministic-слой как baseline, а не как истину.
- API ошибки/таймауты:
  - смягчение: добавить retry/backoff и partial progress writing.

## Что расширять после MVP

- локальная валидация `record_v1` JSON Schema;
- continuation mode (не падать на одном conversation, писать ошибки отдельно);
- batching и/или worker pool;
- офлайн evaluation dataset для regression-тестов качества аннотации.
